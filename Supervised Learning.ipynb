{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114bf46e-3a19-44cc-ac3d-e4a08eadea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the breast cancer dataset\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f33b28-6e51-44db-8704-041f8e231fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Converting the dataset into a DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Displaying first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Checking for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2c3f5b-3858-419c-8a79-620e6fe5e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Handling missing values (in this case, there are no missing values)\n",
    "# Feature Scaling (important for SVM, k-NN, and Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191df559-7a3f-4923-b3ae-55207d1d2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Standard Scaler to features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Spliting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec868d1-fa6d-4561-8f7e-0c0d5b8f3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation of Preprocessing:\n",
    "\n",
    "Missing values: The dataset does not have any missing values, since isnull().sum() has shown zeros. \n",
    "Feature Scaling: We used StandardScaler to scale the features. This is important for algorithms like Logistic Regression, SVM, and k-NN since these algorithms are sensitive to the scale of the data. Random Forest and Decision Tree are not sensitive to feature scaling, but it’s still a good practice to scale all features to a common range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e073d-1094-4faa-bf97-c97a4dfbedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Classification Algorithm Implementation \n",
    "\n",
    "a) Logistic Regression\n",
    "Logistic Regression is a linear model used for binary classification. It outputs, probabilities using the logistic function. It works well when the data is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0d015f-319a-44d6-bb3e-48a73f41b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_log_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1050f8-7669-4c55-a5e4-875b0d1b4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) Decision Tree Classifier\n",
    "A Decision Tree recursively splits the data into smaller subsets based on the most significant feature. It works well for both classification and regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9131c09-615d-4049-973b-225d32488905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "y_pred_dt = dt_classifier.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98304f87-962f-4f94-9633-aa180a8dc5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Random Forest Classifier\n",
    "Random Forest is an ensemble method that builds multiple decision trees and merges their results to improve classification accuracy. It is robust and works well on a variety of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4495d09c-1b19-4f6f-be65-e4845bba9d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96438b2-c9a4-46ba-bfe0-936d594cb664",
   "metadata": {},
   "outputs": [],
   "source": [
    "d) Support Vector Machine (SVM)\n",
    " SVM is a powerful classification algorithm that finds the hyperplane that best separates the data. It works well with high-dimensional data and is especially good when the data is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db579c1-f55c-434d-8a16-bb8787904ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f1c22-a90a-4d16-a18f-8fd17c3b9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "e) k-Nearest Neighbors (k-NN)\n",
    "k-NN is a simple algorithm that classifies data based on the majority class among the k-nearest neighbors. It’s a non-parametric algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da70525a-5a55-43a8-8c00-5dd82e068547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# k-Nearest Neighbors (k-NN)\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "y_pred_knn = knn_classifier.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"k-NN Accuracy: {accuracy_knn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef49d9-7394-46f7-b457-ac1ee5182bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Model Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7512ce19-d8e0-4cfe-9c5b-e37c314453f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.9737\n",
      "SVM: 0.9737\n",
      "Random Forest: 0.9649\n",
      "Decision Tree: 0.9474\n",
      "k-NN: 0.9474\n",
      "\n",
      "Best Model: Logistic Regression with accuracy 0.9737\n",
      "Worst Model: k-NN with accuracy 0.9474\n"
     ]
    }
   ],
   "source": [
    "# Collecting the accuracies of each model\n",
    "accuracies = {\n",
    "    'Logistic Regression': accuracy_log_reg,\n",
    "    'Decision Tree': accuracy_dt,\n",
    "    'Random Forest': accuracy_rf,\n",
    "    'SVM': accuracy_svm,\n",
    "    'k-NN': accuracy_knn\n",
    "}\n",
    "\n",
    "# Sorting the accuracies\n",
    "sorted_accuracies = sorted(accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Displaying the results\n",
    "for model, accuracy in sorted_accuracies:\n",
    "    print(f\"{model}: {accuracy:.4f}\")\n",
    "\n",
    "# Determine the best and worst models\n",
    "best_model = sorted_accuracies[0]\n",
    "worst_model = sorted_accuracies[-1]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model[0]} with accuracy {best_model[1]:.4f}\")\n",
    "print(f\"Worst Model: {worst_model[0]} with accuracy {worst_model[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6e144-6359-4b9e-bb45-abb0f2aa1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary of the Steps:\n",
    "Data Loading and Preprocessing: Loaded the dataset, checked for missing values, scaled the features, and split the data into training and testing sets.\n",
    "Classification Algorithms: Implemented five classification models (Logistic Regression, Decision Tree, Random Forest, SVM, and k-NN) and evaluated their performance using accuracy.\n",
    "Model Comparison: Compared the models' performances based on accuracy and identified the best and worst models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4243b-7165-4da4-9670-c9abbfd0dff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
